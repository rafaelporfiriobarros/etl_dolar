# Projeto ETL de CotaÃ§Ã£o do DÃ³lar (USD/BRL) com Airflow, PostgreSQL e Docker

Este projeto implementa um pipeline **ETL completo** utilizando **Apache Airflow**, **PostgreSQL**, **Docker**, **Python** e um modelo de **Data Lake (Bronze â†’ Silver â†’ Gold)**.
O pipeline captura a cotaÃ§Ã£o diÃ¡ria do dÃ³lar via **AwesomeAPI**, transforma os dados e os armazena de forma estruturada no banco de dados para consultas analÃ­ticas.

---

# Arquitetura do Projeto

```
API (AwesomeAPI)
        â”‚
        â–¼
[EXTRACT] â†’ Bronze (JSON)
        â”‚
        â–¼
[TRANSFORM] â†’ Silver (CSV)
        â”‚
        â–¼
[LOAD] â†’ Gold (PostgreSQL)
        â”‚
        â–¼
Dashboard / Consultas
```

---

# ğŸ“ Estrutura de DiretÃ³rios

```
etl_dolar/
â”‚â”€â”€ dags/
â”‚   â””â”€â”€ etl_dolar.py
â”‚â”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”‚â”€â”€ tests/
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ test_load.py
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ .env
â”‚â”€â”€ README.md
```

---

# Como Rodar o Projeto do Zero (VSCode + Docker)

## **1. Subir o PostgreSQL**

```bash
docker compose up -d postgres
```

Aguarde alguns segundos.

## **2. Inicializar o banco do Airflow**

```bash
docker compose run --rm airflow-webserver airflow db init
```

## **3. Criar usuÃ¡rio admin**

```bash
docker compose run --rm airflow-webserver airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
```

## **4. Subir todo o ambiente**

```bash
docker compose up -d
```

## **5. Acessar o Airflow**

Abra no navegador:

```
http://localhost:8080
```

UsuÃ¡rio: `admin`
Senha: `admin`

## **6. Executar o DAG**

No Airflow:

* Ative o DAG `etl_dolar`
* Clique em **Trigger DAG**
* Acompanhe pelo Graph View

## **7. Consultar os dados no PostgreSQL (via DBeaver)**

```sql
SELECT * FROM cotacao_dolar ORDER BY timestamp DESC LIMIT 20;
```

---

# ExplicaÃ§Ã£o dos Scripts

A seguir, uma explicaÃ§Ã£o clara e profissional de cada parte do projeto.

---

# `scripts/extract.py` â€” Camada Bronze

ResponsÃ¡vel por **extrair** os dados diretamente da AwesomeAPI.

### FunÃ§Ãµes principais:

* Chama a URL definida em `API_URL` (do `.env`)
* Baixa os Ãºltimos registros da cotaÃ§Ã£o do dÃ³lar
* Salva o resultado em um arquivo JSON na pasta `data/bronze/`
* Retorna o caminho do arquivo bruto gerado

### Pontos importantes:

* Traz logs profissionais para acompanhar o processo
* Valida o status da API
* Cria o diretÃ³rio Bronze caso nÃ£o exista

---

# `scripts/transform.py` â€” Camada Silver

ResponsÃ¡vel por **transformar** o JSON bruto em um dataset estruturado.

### Etapas:

* LÃª o arquivo Bronze
* Converte em DataFrame
* Converte colunas numÃ©ricas corretamente
* Converte o timestamp UNIX para datetime
* Gera arquivo CSV na pasta `data/silver/`
* Retorna o caminho do CSV

Essa etapa implementa a **padronizaÃ§Ã£o dos dados**, como acontece em pipelines reais.

---

# `scripts/load.py` â€” Camada Gold (PostgreSQL)

ResponsÃ¡vel por **carregar** o arquivo Silver dentro do banco PostgreSQL.

### O que faz:

* LÃª o CSV Silver
* Conecta ao banco via SQLAlchemy
* Cria ou substitui a tabela `cotacao_dolar`
* Carrega todos os dados transformados
* Gera logs informativos

Essa Ã© a camada **Gold**, onde ficam os dados limpos e prontos para anÃ¡lise.

---

# Testes Automatizados (`tests/`)

Os testes garantem que o pipeline funcione **mesmo sem rodar o Airflow**.

### `test_extract.py`

Valida:

* Se a API estÃ¡ funcional
* Se o arquivo Bronze Ã© criado corretamente

### `test_transform.py`

Valida:

* Se a transformaÃ§Ã£o cria o arquivo Silver
* Se o DataFrame Ã© carregado corretamente

### `test_load.py`

Valida:

* ExtraÃ§Ã£o â†’ TransformaÃ§Ã£o â†’ Carga
* Se o banco aceita os dados corretamente

Esses testes deixam o projeto com um nÃ­vel **profissional**, ideal para portfÃ³lio.

---

# `.env` â€” VariÃ¡veis de Ambiente

Arquivo utilizado para separar configuraÃ§Ãµes sensÃ­veis:

```
API_URL=https://economia.awesomeapi.com.br/json/daily/USD-BRL/30
PG_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
```

O `docker-compose.yml` carrega automaticamente esse arquivo.

---

# `docker-compose.yml`

Esse arquivo define todo o ambiente:

* Container do PostgreSQL
* Containers do Airflow (webserver + scheduler)
* Volumes persistentes
* Montagem das pastas do projeto (dags, scripts, data)
* Carregamento automÃ¡tico do `.env`

Ã‰ o coraÃ§Ã£o da infraestrutura do projeto.

---

# `dags/etl_dolar.py`

Esse arquivo define o **DAG do Airflow**, com trÃªs tasks:

1. `extract` â†’ roda `extract_dolar()`
2. `transform` â†’ roda `transform_dolar()`
3. `load` â†’ roda `load_dolar()`

Ele define:

* Agendamento diÃ¡rio (`@daily`)
* Data de inÃ­cio
* Sem catchup
* DependÃªncias entre as tasks

---

# Resultado Final

Ao final da execuÃ§Ã£o teremos:

### JSON bruto (Bronze)

### CSV organizado (Silver)

### Tabela `cotacao_dolar` no PostgreSQL (Gold)

Isso permite anÃ¡lises, dashboards e consultas SQL de forma estruturada.

---
